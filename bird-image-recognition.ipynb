{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 32\n",
    "# test split\n",
    "test_size = 0.2\n",
    "\n",
    "img_dim = (224, 224)\n",
    "\n",
    "# paths\n",
    "cache_path = 'cache'\n",
    "data_path = 'data-filtered'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images for 'Aberrant Bush-Warbler'\n",
      "270 images loaded for 'Aberrant Bush-Warbler'\n",
      "Loading images for 'Ala Shan Redstart'\n",
      "401 images loaded for 'Ala Shan Redstart'\n",
      "Loading images for 'Aleutian Tern'\n",
      "282 images loaded for 'Aleutian Tern'\n",
      "Loading images for 'Altai Snowcock'\n",
      "244 images loaded for 'Altai Snowcock'\n",
      "Loading images for 'American Wigeon'\n",
      "486 images loaded for 'American Wigeon'\n",
      "Loading images for 'Arctic Warbler'\n",
      "329 images loaded for 'Arctic Warbler'\n",
      "Loading images for 'Ashy Bulbul'\n",
      "310 images loaded for 'Ashy Bulbul'\n",
      "Loading images for 'Ashy Drongo'\n",
      "412 images loaded for 'Ashy Drongo'\n",
      "Loading images for 'Ashy Minivet'\n",
      "443 images loaded for 'Ashy Minivet'\n",
      "Loading images for 'Ashy Wood Pigeon'\n",
      "335 images loaded for 'Ashy Wood Pigeon'\n",
      "Loading images for 'Ashy Woodswallow'\n",
      "306 images loaded for 'Ashy Woodswallow'\n",
      "Loading images for 'Ashy-throated Parrotbill'\n",
      "252 images loaded for 'Ashy-throated Parrotbill'\n",
      "Loading images for 'Ashy-throated Warbler'\n",
      "442 images loaded for 'Ashy-throated Warbler'\n",
      "Loading images for 'Asian Brown Flycatcher'\n",
      "463 images loaded for 'Asian Brown Flycatcher'\n",
      "Loading images for 'Asian Dowitcher'\n",
      "410 images loaded for 'Asian Dowitcher'\n",
      "Loading images for 'Asian Fairy-bluebird'\n",
      "311 images loaded for 'Asian Fairy-bluebird'\n",
      "Loading images for 'Asian House-Martin'\n",
      "238 images loaded for 'Asian House-Martin'\n",
      "Loading images for 'Asian Pied Starling'\n",
      "354 images loaded for 'Asian Pied Starling'\n",
      "Loading images for 'Asian Rosy-Finch'\n",
      "428 images loaded for 'Asian Rosy-Finch'\n",
      "Loading images for 'Asian Stubtail'\n",
      "201 images loaded for 'Asian Stubtail'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "n_birds = 10 # number of kinds of birds to categorize\n",
    "bird_file_map = {}\n",
    "least_image_n = 1\n",
    "\n",
    "# return array of bird names\n",
    "birdList = sorted(os.listdir(data_path))\n",
    "loadedImages = []\n",
    "\n",
    "n_images = 0\n",
    "n_birds_loaded = 0\n",
    "for b in birdList:\n",
    "    if n_birds_loaded >= n_birds:\n",
    "        break\n",
    "    print(\"Loading images for '\" + b + \"'\")\n",
    "    curdir = os.path.join(data_path, b)\n",
    "    if not os.path.isdir(curdir):\n",
    "        continue\n",
    "    img_files = os.listdir(curdir)\n",
    "\n",
    "    filenames = [os.path.join(curdir, f) for f in img_files]\n",
    "    n_f = len(filenames)\n",
    "    if n_f >= least_image_n: # use data only if more than xx images are found\n",
    "        bird_file_map[b] = filenames\n",
    "        print(n_f, \"images loaded for '\" + b + \"'\")\n",
    "        n_birds_loaded += 1\n",
    "        n_images += n_f\n",
    "    else:\n",
    "        print(\"Not enough data for '\" + b + \"'\")\n",
    "\n",
    "n_birds = len(bird_file_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "n_channels = 3\n",
    "X = np.zeros((n_images, img_dim[0], img_dim[1], n_channels))\n",
    "labels = []\n",
    "\n",
    "i = 0\n",
    "for k, v in bird_file_map.items():\n",
    "    for file in v:\n",
    "        try:\n",
    "            im = cv2.imread(file)\n",
    "            if im is None or im.shape[0] < img_dim[0] or im.shape[1] < img_dim[1]:\n",
    "                continue\n",
    "            shape = im.shape\n",
    "            assert len(shape) == 3 # width, length and color channels\n",
    "            assert shape[-1] == 3 # rgb, three channels\n",
    "            \n",
    "            # resizing\n",
    "            im = cv2.resize(src=im, dsize=img_dim, interpolation=cv2.INTER_LINEAR)\n",
    "            # Gaussian blurring\n",
    "            # im = cv2.GaussianBlur(im, (5, 5), 0)\n",
    "            X[i] = np.asarray(im)\n",
    "            labels.append(k)\n",
    "            i += 1\n",
    "        except IOError:\n",
    "            continue\n",
    "n_images = len(labels)\n",
    "X = X[:n_images, ]\n",
    "del i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_unique = list(set(labels))\n",
    "\n",
    "Y = np.zeros((n_images, 1))\n",
    "for i in range(n_images):\n",
    "    Y[i, 0] = labels_unique.index(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3790, 224, 224, 3)\n",
      "(3790, 1)\n",
      "0.0 255.0\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(np.min(X), np.max(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(X.shape[0]):\n",
    "    m = np.min(X[i,])\n",
    "    X[i] = (X[i,] - m) / (np.max(X[i,]) - m)\n",
    "\n",
    "print(np.min(X), np.max(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing memory size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1088.14 Mb (75.0% reduction)\n",
      "Mem. usage decreased to  0.01 Mb (75.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Function to reduce the numpy array size\n",
    "def reduce_mem_usage(a, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = a.nbytes / 1024**2\n",
    "    dtype = a.dtype\n",
    "    if dtype in numerics:\n",
    "        c_min = a.min()\n",
    "        c_max = a.max()\n",
    "        if str(dtype)[:3] == 'int':\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                a = a.astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                a = a.astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                a = a.astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                a = a.astype(np.int64)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                a = a.astype(np.float16)\n",
    "            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                a = a.astype(np.float32)\n",
    "            else:\n",
    "                a = a.astype(np.float64)\n",
    "    end_mem = a.nbytes / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return a\n",
    "\n",
    "X = reduce_mem_usage(X)\n",
    "Y = reduce_mem_usage(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3032, 224, 224, 3) | (758, 224, 224, 3)\n",
      "(3032, 20) | (758, 20)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "# y_valid = to_categorical(y_valid)\n",
    "print(X_train.shape, '|', X_test.shape)\n",
    "print(y_train.shape, '|', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data augmentation sucessful, 3032 new images in total were added\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "img_gen_batch_size = 32\n",
    "image_gen_train = ImageDataGenerator(\n",
    "                    rotation_range=45,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.1)\n",
    "\n",
    "image_gen_flow = image_gen_train.flow(X_train, y_train, batch_size=img_gen_batch_size)\n",
    "X_added = np.zeros((len(image_gen_flow) * img_gen_batch_size, *(X_train.shape[1:])))\n",
    "Y_added = np.zeros((len(image_gen_flow) * img_gen_batch_size, *(y_train.shape[1:])))\n",
    "flow_len = len(image_gen_flow)\n",
    "\n",
    "n_added = 0\n",
    "for i in range(0, flow_len):\n",
    "    X_batch = image_gen_flow[i][0]\n",
    "    img_gen_batch_size = X_batch.shape[0]\n",
    "    X_added[i * img_gen_batch_size: (i + 1) * img_gen_batch_size,] = X_batch\n",
    "    Y_added[i * img_gen_batch_size: (i + 1) * img_gen_batch_size,] = image_gen_flow[i][1]\n",
    "    n_added += img_gen_batch_size\n",
    "print('data augmentation sucessful, {} new images in total were added'.format(n_added))\n",
    "del flow_len\n",
    "del X_batch\n",
    "del img_gen_batch_size\n",
    "X_train = np.vstack([X_train, X_added])\n",
    "y_train = np.vstack([y_train, Y_added])\n",
    "del X_added\n",
    "del Y_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 1D or 2D array, got 4D array instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6e58021b8325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x_test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1382\u001b[0;31m                 \"Expected 1D or 2D array, got %dD array instead\" % X.ndim)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0;31m# Common case -- 1d array of numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 1D or 2D array, got 4D array instead"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "f = open('x_train.pkl', 'wb')\n",
    "pickle.dump(X_train, f)\n",
    "f.close()\n",
    "\n",
    "f = open('y_train.pkl', 'wb')\n",
    "pickle.dump(y_train, f)\n",
    "f.close()\n",
    "\n",
    "f = open('x_test.pkl', 'wb')\n",
    "pickle.dump(X_test, f)\n",
    "f.close()\n",
    "\n",
    "f = open('y_test.pkl', 'wb')\n",
    "pickle.dump(y_test, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
